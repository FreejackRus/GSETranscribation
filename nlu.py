# nlu.py ‚Äì —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ —Ñ—É–Ω–∫—Ü–∏–π
import json
import re
import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

logger = logging.getLogger(__name__)


def load_llm_model(model_name: str):
    """load_llm_model ‚Äì –æ—Å—Ç–∞–≤–ª—è–µ–º –∏–º—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π."""
    logger.info("üß† –ó–∞–≥—Ä—É–∂–∞–µ–º LLM: %s", model_name)
    tok = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto",
    )
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ %s", model.device)
    return tok, model


def extract_first_json(text: str):
    """extract_first_json ‚Äì —Å—Ç–∞—Ä–æ–µ –∏–º—è."""
    match = re.search(r'\{.*\}', text, flags=re.S)
    if not match:
        return None
    try:
        return json.loads(match.group(0))
    except json.JSONDecodeError:
        return None


def extract_data_from_text_fallback(text: str):
    """extract_data_from_text_fallback ‚Äì —Å—Ç–∞—Ä–æ–µ –∏–º—è."""
    data = {}

    # –ø–æ–µ–∑–¥
    train_match = re.search(r"(?:–ø–æ–µ–∑–¥|train)\s*(?:‚Ññ|#)?\s*(\d+)", text, re.I)
    data["train_number"] = train_match.group(1) if train_match else None

    # –≤–∞–≥–æ–Ω
    m = re.search(r"(?:–≤–∞–≥–æ–Ω|wagon)\s*(?:‚Ññ|#)?\s*(\d{1,2})", text, re.I)
    data["wagon_number"] = m.group(1) if m else None

    # —Å–µ—Ä–∏–π–Ω–∏–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é None)
    data["wagon_sn"] = None

    # –§–ò–û
    m = re.search(r"\b([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)?)\b", text)
    data["executor_name"] = m.group(1) if m else None

    # –ø—Ä–æ–±–ª–µ–º—ã
    probs = re.findall(
        r"(?:–ø—Ä–æ–±–ª–µ–º–∞|–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç|—Å–ª–æ–º–∞–Ω|–æ—Ç–∫–ª—é—á|–ø—Ä–æ–ø–∞–¥–∞–µ—Ç|–Ω–µ –∏–¥[–µ—ë]—Ç)\s*[–∞-—è—ëa-z0-9,\s\-]{3,}",
        text, re.I
    )
    data["problems"] = [p.strip() for p in probs] or [text[:120].strip()]
    return data


def parse_voice_claim(text: str, tokenizer, model):
    """parse_voice_claim ‚Äì —Å—Ç–∞—Ä–æ–µ –∏–º—è."""
    prompt = (
        '–ò–∑–≤–ª–µ–∫–∏ –¥–∞–Ω–Ω—ã–µ –∏ –≤–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ JSON: {"train_number":"", "wagon_number":"", '
        '"wagon_sn":"", "problems":[], "executor_name":""}\n\n'
        '- train_number ‚Äì —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã, –±–µ–∑ –±—É–∫–≤ –∏ —Å–ª–æ–≤ '
        f'–¢–µ–∫—Å—Ç: "{text}"\n'
    )
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        ids = model.generate(
            **inputs,
            max_new_tokens=120,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=False,
        )
    response = tokenizer.decode(ids[0], skip_special_tokens=True)[len(prompt):].strip()

    result = extract_first_json(response)
    if result:
        data = {k: result.get(k) for k in ("train_number", "wagon_number", "wagon_sn", "problems", "executor_name")}
        data["problems"] = list(map(str, data.get("problems") or []))
        logger.info("‚úÖ JSON –∏–∑ LLM –ø–æ–ª—É—á–µ–Ω")
    else:
        data = extract_data_from_text_fallback(text)
        logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback")

    return data